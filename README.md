# Reproducibility Package â€“ Language Models as Architectural Gatekeepers

This repository contains all datasets, scripts, and supplementary material associated with the paper:

> **Language Models as Architectural Gatekeepers: Automating Conformance Checking from Natural Language**  
> *Andrielly Lucena, Everton L. G. Alves, JoÃ£o Brunet*  
> *Accepted at the Brazilian Symposium on Software Engineering (SBES 2025)*

The PDF version of the paper can be found in the file [`paper.pdf`](./paper.pdf) included in this repository.

---

## Abstract

Ensuring architectural conformance is a key challenge in software engineering. This work explores the use of Large Language Models (LLMs) to generate **design conformance tests** directly from **natural language design rules**, typically expressed in development discussions (e.g., code reviews, issues). Our approach avoids the need for formal architecture specifications by leveraging informal communication and transforming it into executable design tests using the [Design Wizard](https://github.com/joaoarthurbm/designwizard) library.

---

## Repository Structure

```

.
â”œâ”€â”€ codebase/                     # Java code (DesignPatterns) used in evaluation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ doc/                      # HTML documentation from Design Wizard used in RAG
â”‚   â”œâ”€â”€ rules.csv                 # Rules + context used in test generation
â”‚   â”œâ”€â”€ generated-design-tests/  # Java code generated by LLM for each rule
â”œâ”€â”€ designwizard/                # .jar of the Design Wizard library
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ constants.py             # Utility file with shared constants
â”‚   â”œâ”€â”€ rule\_generator.py        # Script to generate realistic natural language rules using GPT
â”‚   â”œâ”€â”€ vector\_creation.py       # Loads documentation into PostgreSQL for RAG
â”‚   â”œâ”€â”€ query.py                 # Main test generator: queries the doc + generates test from rule
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENCE
â””â”€â”€ README.md


```
---

## Environment Setup

To reproduce the experiments, make sure you have the following installed:

- **Python** 3.13.3
- **Java** 8 (specifically Java 8, not newer versions)
- **Docker** (for running PostgreSQL with pgvector)
- **PostgreSQL** (will be used via Docker)

Before running the scripts, configure the environment variables:

1. Copy the example environment file:

   ```bash
   cp .env.example .env
    ```

2. Then, edit the `.env` file and add your API keys:

   - `OPENAI_API_KEY` â€“ [Get your key here](https://platform.openai.com/account/api-keys)
   - `MISTRAL_API_KEY` â€“ [Get your key here](https://console.mistral.ai/api-keys)

---

### Python Requirements Installation

It is recommended to use a Python virtual environment (e.g., with `venv`):

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

### Set up PostgreSQL with pgvector

You must be running a PostgreSQL database with `pgvector` support. To start a container with the required setup:

```bash
docker run --name pgvector-db \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_DB=designwizard_docs \
  -p 5432:5432 \
  -d pgvector/pgvector:pg16
```

## Running the Pipeline

### 1. Ingest Design Wizard documentation

The documentation used for the RAG pipeline is located in `data/doc/`. To store it in the database as vector embeddings, run:

```bash
python scripts/vector_creation.py
```

---

### 2. Generate Natural Language Rules

To generate design rules in natural language from real-world discussion examples:

```bash
python scripts/rule_generator.py
```

The result will be saved in `output/rules.txt`.

> **Note:** Not all rules generated by the LLM are used in the study. You must manually select which rules will be used in the next steps.

> **Tip:** The prompt at the beginning of `scripts/rule_generator.py` can be modified to generate rules for any of the 6 design patterns in the study (Strategy, Facade, Simple Factory, Composite, Adapter, Builder).

---

### 3. Select and Add Rules for Test Generation

After manually selecting the desired rules (from `output/rules.txt`), add them as an array named `rules` in the file `scripts/query.py`:

```python
rules = [
    "Rule 1...",
    "Rule 2...",
    # ...
]
```

The script will iterate over this array and generate a design test for each rule. The generated tests (model output) will be saved in a file in the `output` folder, named `saida-{date and time of execution}.txt`.

---

### 4. Execute the Generated Design Tests

To execute the generated design tests:

1. Copy the Java test code(s) from the output file in `output/saida-{date and time}.txt`.
2. Paste the code into `src/test/DesignTest.java` inside the `codebase` folder corresponding to the chosen design pattern.
3. Compile and run using the Java commands already described in the next step.

---

## Evaluation Setup

We used the [DesignPatterns](https://github.com/clarck/DesignPatterns) repository to validate generated tests.

### Java Code Compilation & Execution

1. Navigate into a specific design pattern folder inside `codebase/`.
2. Compile with:

```bash
javac -d bin -cp "lib/junit-4.13.2.jar;lib/hamcrest-core-1.3.jar;lib/designwizard-1.4.jar;." src/test/*.java src/com/cnblog/clarck/*.java
```

3. Execute with:

```bash
java -cp "lib/junit-4.13.2.jar;lib/hamcrest-core-1.3.jar;lib/designwizard-1.4.jar;bin;." org.junit.runner.JUnitCore test.DesignTest
```

---

## Contact

**Andrielly Lucena**
Federal University of Campina Grande (UFCG), Brazil
ðŸ“§ [andriellylucena@copin.ufcg.edu.br](mailto:andriellylucena@copin.ufcg.edu.br)
